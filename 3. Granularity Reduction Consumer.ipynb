{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Week 11 - Granularity Reduction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "Connection to Kafka Producer/Broker and subscribe to the topic and load data from Kafka topic with <code>readStream</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "hostip = \"192.168.1.87\" #change me\n",
    "topic = \"week11_orig_data\"\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{hostip}:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 \n",
    "Converting the value from the kafka data stream to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "Define a schema according to our data (as sent from the producer), Use <code>from_json</code> to parse the string to the json format based on the defined schema.\n",
    "Each message contains the value of the timestamp as \"ts\" field and a random integer value as \"value\" field, you can define a schema as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('ts', TimestampType(), True),\n",
    "    StructField('value', IntegerType(), True)          \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias('parsed_value'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns need to be renamed appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted = df.select(\n",
    "                    F.col(\"parsed_value.ts\").alias(\"ts\"),\n",
    "                    F.col(\"parsed_value.value\").alias(\"value\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Reduce the value of the data by grouping the timestamp \"ts\" on a window of 5 seconds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the window function, we can perform the following aggregation \n",
    "grouped_avg = df_formatted.groupBy(F.window(\"ts\",\"5 second\"))\\\n",
    "                    .agg(F.avg(\"value\").alias(\"avg_value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_avg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and renaming the columns appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_avg = grouped_avg.select(\n",
    "                    F.col(\"window.end\").alias(\"end_time\"),\n",
    "                    F.col(\"avg_value\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_avg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "Create the <strong>output sink</strong> for the stream. For this case, we will output the data in memory. One will output the original random values in a table called  \"query_all\" and the reduced values in another table called \"reduced_values\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the output sink to \"memory\" and write output to the memory sink\n",
    "query_all = df_formatted \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"all_values\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the output sink to \"memory\" and write output to the memory sink\n",
    "query_reduced = grouped_avg \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"reduced_values\") \\\n",
    "    .trigger(processingTime='5 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from all_values order by ts asc\").show()\n",
    "spark.sql(\"select * from reduced_values order by end_time asc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing streaming data \n",
    "We have implemented the aggregation to get the average values of the random data in a window of 5 seconds. Let's write this this to the memory sink and query it using spark sql for visualizing it in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, first we need to initialize an empty plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_plots():\n",
    "    try:\n",
    "        width = 9.5\n",
    "        height = 6\n",
    "        fig = plt.figure(figsize=(width,height)) # create new figure\n",
    "        fig.subplots_adjust(hspace=0.8)\n",
    "        ax = fig.add_subplot(111) # adding the subplot axes to the given grid position\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.title.set_text('Time Vs Value')\n",
    "        fig.suptitle('Real-time uniform stream data visualization') # giving figure a title\n",
    "        fig.show() # displaying the figure\n",
    "        fig.canvas.draw() # drawing on the canvas\n",
    "        return fig, ax\n",
    "    except Exception as ex:\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = init_plots()\n",
    "\n",
    "while True:\n",
    "    df_all = spark.sql(\"select * from all_values order by ts desc limit 90\").toPandas()\n",
    "    # Get starting timestamp to plot both graphs\n",
    "    start_time = df_all['ts'][len(df_all)-1]\n",
    "    df_reduced = spark.sql(\"select * from reduced_values where end_time>='\"+str(start_time)+\"' order by end_time desc\").toPandas()\n",
    "    \n",
    "    x_all = df_all['ts'].to_list()\n",
    "    y_all = df_all['value'].to_list()\n",
    "    x_reduced = df_reduced['end_time'].to_list()\n",
    "    y_reduced = df_reduced['avg_value'].to_list()\n",
    "    ax.clear()\n",
    "    ax.plot(x_all, y_all, '-b', label='Original')\n",
    "    ax.plot(x_reduced, y_reduced, '--r', label='Reduced')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    leg = ax.legend()\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
